{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from airflow import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom datetime import datetime, timedelta\\n\\ndefault_dag_arg = {\\n    \\'owner\\': \\'lorenzo\\',\\n    \\'start_date\\': datetime(2023, 10, 8),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes= 5),\\n    \\'project_id\\': 1\\n}\\n\\nwith DAG(\\n    dag_id= \\'copy lcean data\\', #ID univoco per la mia DAG\\n    description= \\'Copia i file dalla directory data_lake alla directory clean_data\\',\\n    schedule_interval= None,\\n    default_args= default_dag_arg) as dag:\\n    \\n    task_copy = BashOperator(\\n        task_id= \\'copy_file\\',\\n        bash_command=  \"cp /home/unix/data_center/data_lake/data.txt /home/unix/data_center/clean_data\",\\n        dag= dag\\n    )'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_dag_arg = {\n",
    "    'owner': 'lorenzo',\n",
    "    'start_date': datetime(2023, 10, 8),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes= 5),\n",
    "    'project_id': 1\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id= 'copy lcean data', #ID univoco per la mia DAG\n",
    "    description= 'Copia i file dalla directory data_lake alla directory clean_data',\n",
    "    schedule_interval= None,\n",
    "    default_args= default_dag_arg) as dag:\n",
    "    \n",
    "    task_copy = BashOperator(\n",
    "        task_id= 'copy_file',\n",
    "        bash_command=  \"cp /home/unix/data_center/data_lake/data.txt /home/unix/data_center/clean_data\",\n",
    "        dag= dag\n",
    "    )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\n\\n# Define Python function\\n\\ndef hello():\\n    print(\\'Hello\\')\\n    \\n# Define default arguments \\n\\ndefault_args = {\\n    \\'owner\\': \\'lorenzo\\',\\n    \\'start_date\\': datetime(2023, 10, 10),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes= 5)\\n}\\n\\n# Defaine the DAG \\n\\nwith DAG(\\n    dag_id= \\'second_dag\\',\\n    description= \"Execute a PY function to print \\'Hello\\'\",\\n    default_args= default_args,\\n    schedule_interval= None) as dag:\\n    \\n    task_py = PythonOperator(\\n        task_id= \\'second_dag\\',\\n        python_callable= hello\\n    )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Define Python function\n",
    "\n",
    "def hello():\n",
    "    print('Hello')\n",
    "    \n",
    "# Define default arguments \n",
    "\n",
    "default_args = {\n",
    "    'owner': 'lorenzo',\n",
    "    'start_date': datetime(2023, 10, 10),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes= 5)\n",
    "}\n",
    "\n",
    "# Defaine the DAG \n",
    "\n",
    "with DAG(\n",
    "    dag_id= 'second_dag',\n",
    "    description= \"Execute a PY function to print 'Hello'\",\n",
    "    default_args= default_args,\n",
    "    schedule_interval= None) as dag:\n",
    "    \n",
    "    task_py = PythonOperator(\n",
    "        task_id= 'second_dag',\n",
    "        python_callable= hello\n",
    "    )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom datetime import datetime, timedelta\\nimport requests\\nimport json\\n\\n#Define PY function\\ndef market_data():\\n    \"\"\"\\n    Fetches weekly market data for IBM from the Alphavantage API and saves it as a JSON file.\\n\\n    The function constructs the API request URL using a predefined API key. It then sends a GET request\\n    to the API. If the response status is 200 (OK), the JSON data is written to a specified file path.\\n    If the response status is anything other than 200, an exception is raised.\\n\\n    Args:\\n        None\\n\\n    Returns:\\n        None\\n    \"\"\"\\n    api_key = \\'SWSFGG5JHXFNNZAO\\'\\n    url = f\\'https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol=IBM&apikey={api_key}\\'\\n    response = requests.get(url)\\n    \\n    if response.status_code != 200:\\n        raise Exception(f\\'Failed request. Status {response.status_code}\\')\\n    \\n    data = response.json()\\n    \\n    with open(\\'/home/unix/data_center/data_lake/market_data.json\\', \\'w\\') as file:\\n        json.dump(data, file)\\n    \\n#Define default_arg\\n\\ndefault_args = {\\n    \\'owner\\': \\'lorenzo\\',\\n    \\'start_date\\': datetime(2023, 10, 9),\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\': False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\': timedelta(minutes= 5)\\n}\\n\\n# Define DAG\\nwith DAG(\\n    dag_id= \\'market_data\\',\\n    default_args= default_args,\\n    description= \\'Fetch market data from Alphavantage API\\',\\n    schedule_interval= None,\\n    catchup= False) as dag:\\n    \\n    # Define Task\\n    task_fetch_market_data = PythonOperator(\\n        task_id= \\'request_market_data\\',\\n        python_callable= market_data\\n        dag= dag\\n    )'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Define PY function\n",
    "def market_data():\n",
    "    \"\"\"\n",
    "    Fetches weekly market data for IBM from the Alphavantage API and saves it as a JSON file.\n",
    "\n",
    "    The function constructs the API request URL using a predefined API key. It then sends a GET request\n",
    "    to the API. If the response status is 200 (OK), the JSON data is written to a specified file path.\n",
    "    If the response status is anything other than 200, an exception is raised.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    api_key = 'SWSFGG5JHXFNNZAO'\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol=IBM&apikey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed request. Status {response.status_code}')\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    with open('/home/unix/data_center/data_lake/market_data.json', 'w') as file:\n",
    "        json.dump(data, file)\n",
    "    \n",
    "#Define default_arg\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'lorenzo',\n",
    "    'start_date': datetime(2023, 10, 9),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes= 5)\n",
    "}\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    dag_id= 'market_data',\n",
    "    default_args= default_args,\n",
    "    description= 'Fetch market data from Alphavantage API',\n",
    "    schedule_interval= None,\n",
    "    catchup= False) as dag:\n",
    "    \n",
    "    # Define Task\n",
    "    task_fetch_market_data = PythonOperator(\n",
    "        task_id= 'request_market_data',\n",
    "        python_callable= market_data\n",
    "        dag= dag\n",
    "    )'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92907/2180652854.py:4: DeprecationWarning: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `'airflow.operators.python.PythonOperator'`.\n",
      "  from airflow.operators.python_operator import PythonOperator\n",
      "/tmp/ipykernel_92907/2180652854.py:21: RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead. \n",
      "  with DAG(\"first_python_dag\",\n"
     ]
    }
   ],
   "source": [
    "#Exercise - 02\n",
    "from datetime import datetime, timedelta \n",
    "from airflow import DAG \n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "#create a dag with one python task only. This function should print the current datetime\n",
    "\n",
    "def python_first_function(): \n",
    "    today = datetime.now().date()\n",
    "    print(today)\n",
    "    \n",
    "#create the DAG which calls the python logic that you had created above\n",
    "default_dag_args = {'start_date': datetime(2022, 9, 1), \n",
    "                    'email_on_failure': False, \n",
    "                    'email_on_retry': False, \n",
    "                    'retries': 1, \n",
    "                    'retry_delay': timedelta(minutes=5), \n",
    "                    'project_id': 1 }\n",
    "\n",
    "#crontab notation can be useful https://crontab.guru/#0_0_*_*_1\n",
    "with DAG(\"first_python_dag\", \n",
    "         schedule_interval = '@daily', \n",
    "         catchup=False, \n",
    "         default_args = default_dag_args) as dag_python:\n",
    "\n",
    "    # here we define our tasks\n",
    "    task_0 = PythonOperator(task_id = \"first_python_task\", \n",
    "                            python_callable = python_first_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import time \n",
    "import json \n",
    "from airflow import DAG \n",
    "from airflow.operators.python_operator import PythonOperator \n",
    "from airflow.operators.python import BranchPythonOperator \n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "# exercise: write a DAG which is able to request market data for a list of stocks.\n",
    "\n",
    "\"\"\"this list should be an input of your function. The functions names are left to help you\n",
    "you DAG should only have one task\"\"\"\n",
    "\n",
    "def get_data(**kwargs): \n",
    "    pass\n",
    "\n",
    "# create the DAG which calls the python logic that you had created above\n",
    "default_dag_args = {'start_date': datetime(2023, 10, 9), \n",
    "                    'email_on_failure': False, \n",
    "                    'email_on_retry': False, \n",
    "                    'retries': 1, \n",
    "                    'retry_delay': timedelta(minutes=5), \n",
    "                    'project_id': 1 }\n",
    "\n",
    "\n",
    "# crontab notation can be useful https://crontab.guru/#0_0_*_*_1\n",
    "with DAG(\"market_data_alphavantage_dag\", \n",
    "         schedule_interval = '@daily', \n",
    "         catchup=False, \n",
    "         default_args = default_dag_args) as dag_python:\n",
    "\n",
    "    # here we define our tasks\n",
    "    task_0 = PythonOperator(task_id = \"get_market_data\", \n",
    "                            python_callable = get_data, \n",
    "                            op_kwargs = {'tickers' : []})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
